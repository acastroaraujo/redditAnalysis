---
title: "prawr"
author: "andrés castro araújo"
date: "`r date()`"
output: 
  html_document: 
    code_folding: show
    theme: lumen
    toc: yes
    toc_float: 
      collapsed: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = "center")
```

```{r, message=FALSE}
library(tidyverse); theme_set(theme_minimal(base_family = "Avenir", base_line_size = 0))
library(reticulate)

##### create a new environment 
## conda_create("reddit")
## conda_install("reddit", c("numpy", "praw"))

use_condaenv(condaenv = "reddit")
```

```{r results="asis", echo=FALSE}
cat("<style>
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>")
```


## 1^st^ Info

Load secret info.

```{r}
source("secretInfo.R")
```

## 2^nd^ Download

Download _entire_ thread

```{python}
## Python chunk
import praw

reddit = praw.Reddit(client_id = r.client_id,
                     client_secret = r.client_secret,
                     password = r.password,
                     user_agent = 'Comment Extraction (by acastroaraujo)',
                     username = r.username)
```

```{python}
## Python chunk
reddit.user.me()
```

```{r}
## R chunk
py$reddit$user$me()
```

```{python}
## Python chunk
# thread = "https://www.reddit.com/r/ProgrammerHumor/comments/92eq8q/knowing_python/"
# thread = "https://www.reddit.com/r/datascience/comments/cw5ury/data_analysis_one_of_the_most_important/"

thread = "https://www.reddit.com/r/politics/comments/cw2eqa/trump_keeps_complaining_that_he_never_suggested/"
submission = reddit.submission(url = thread)
```

```{r}
## R chunk
py$submission$author
py$submission$title
py$submission$num_comments
```

```{python}
## Python chunk, gets the rest of the comment forest
## This is what's lacking from the RedditExtractor package!!
## Right now, it's not a problem because the number of comments is below the 500 threshold.
submission.comments.replace_more(limit = None)
```

Comments are stored inside the `py$submission` object (as objects of class `CommentForest`), but the structure of this object requires some explaining.

```{python}
print(submission.comments)
# submission.comments[:] ## All top level comments.
```

```{r}
## R chunk
## All top level comments
py_eval("submission.comments[:]") %>% length()
py_eval("submission.comments[:]")[1:5] ## First five
```

Each of these elements is an environment object. They also contain a lot of stuff, that's very useful for other purposes, but that we can consider junk.

```{r}
py_eval("submission.comments[:]")[[1]] %>% typeof()
py_eval("submission.comments[:]")[[1]] %>% class()
```

This is how we start untangling the comment forest:

1. Second top level comment

```{python}
a = submission.comments.list()[1]
print(a.body)
```

All replies to that comment are accessed through the following procedure:

```{python}
print(len(a.replies.list()), "replies")
print("First five replies:")
a.replies.list()[0:5]
```

But if we are only interested in the first level responses, we use Python's slice operator:

```{python}
print(len(a.replies[:]), "direct replies")
```

2. First reply to the top level comment.

```{python}
print(a.replies[:][0].body)
```

2. First reply to the first reply of the  top level comment.

```{python}
print(a.replies[:][0].replies[:][0].body)
```

3. First reply to the first reply of the first reply of the  top level comment.

```{python}
print(a.replies[:][0].replies[:][0].replies[:][0].body)
```

4. Etc.

```{python}
print(a.replies[:][0].replies[:][0].replies[:][0].replies[:][0].body)
```

We can do something similar in R by either using the `list()` method from the `CommentForest` object:

```{r}
py$a$body
py$a$replies$list()[[1]]$body
py$a$replies$list()[[1]]$replies$list()[[1]]$body
py$a$replies$list()[[1]]$replies$list()[[1]]$replies$list()[[1]]$body
```

Or we can implement Python's `[:]` operator by saving a slice object and reusing it.

```{python}
index = slice(None)
```

```{r}
py$submission$comments[py$index] %>% ## All top level comments
  length()
```

The difference is very easy to see by looking at just a few nodes:

```{r}
thread <- py$submission$comments

for (node in thread$list()[1:5]) {
  cat("Comment", node$id, "has", length(node$replies[py$index]), "children and", 
  length(node$replies$list()), "descendents\n")
}
```

## 3^rd^ Graph

1. Make a Node data frame

```{r}
root <- py$submission ## root node
nodes <- py$submission$comments$list()  ## all nodes

## general purpose functions

get_date <- function(x) {
  as.POSIXct(x$created_utc, origin = "1970-01-01", tz = "UTC")
  }

get_author <- function(x) {
  output <- x$author
  if (is.null(output)) return("[deleted]")
  as.character(output)
}

root_df <- tibble(
  name = root$id,
  author = as.character(root$author),
  date = get_date(root),
  children = length(root$comments[py$index]),
  descendents = length(root$comments$list()),
  text = root$selftext,
  title = root$title,
  media = root$url,
  subreddit = as.character(root$subreddit),
  path = root$permalink
)

glimpse(root_df)


nodes_df <- tibble(
  name = map_chr(nodes, function(x) x$id),
  author = map_chr(nodes, get_author),
  date = map_dbl(nodes, get_date),
  children = map_int(nodes, function(x) length(x$replies[py$index])),
  descendents = map_int(nodes, function(x) length(x$replies$list())),
  text = map_chr(nodes, function(x) x$body),
  score = map_int(nodes, function(x) x$score),
  title = py$submission$title,
  subreddit = as.character(root$subreddit),
  path = root$permalink
  ) %>% 
  mutate(date = as.POSIXct(date, origin="1970-01-01"))

glimpse(nodes_df)
```

```{r}
full_df <- full_join(root_df, nodes_df)
```

```{r}
glimpse(full_df)
```


So far so good, but we still need to figure out a way to build the tree-structure relationship dependencies.

2. Make edge list

To make the edge list, we use the `parent_id` attribute for each node, which gives us the id along with an annoying prefix (which we eliminate using `str_sub()`).

```{r}
output <- vector("list", length(nodes))

for (i in seq_along(output)) {
  output[[i]] <- list(from = nodes[[i]]$parent_id, to = as.character(nodes[[i]]))
}







edge_list <- bind_rows(output) %>%
  mutate(from = str_sub(from, 4))

glimpse(edge_list)
```

## 4^th^ Plot

Trees

```{r}
thread_network <- igraph::graph_from_data_frame(d = edge_list, 
                                                directed = TRUE, 
                                                vertices = full_df)
library(ggraph)
thread_network %>%
  ggraph("dendrogram", circular = FALSE) +
  geom_edge_diagonal(alpha = 0.5, width = 0.1) +
  geom_node_point(size = 0.01) +
  theme_graph()

thread_network %>%
  ggraph("dendrogram", circular = TRUE) +
  geom_edge_diagonal(alpha = 0.5, width = 0.1) +
  geom_node_point(size = 0.01) +
  coord_fixed() +
  theme_graph()
```

```{r}
thread_network %>% 
  ggraph('treemap') + 
  geom_edge_link(alpha = 0.4) + 
  geom_node_point(size = 0.4) +
  theme_graph()

thread_network %>% 
  ggraph('circlepack') + 
  geom_edge_density() + 
  geom_edge_link(alpha = 0.2) +
  geom_node_point(size = 0.2) +
  theme_graph()
```

This one actually looks good, although we might think about removing first level nodes that didn't generate a conversation.

Here we remove all leaves.

```{r}
thread_network %>% 
  tidygraph::as_tbl_graph() %>% 
  mutate(children = igraph::degree(thread_network, mode = "out")) %>% 
  filter(score > 0 | is.na(score)) %>% 
  ggraph('circlepack', circular = TRUE) + 
  geom_edge_link(alpha = 0.2) + 
  geom_node_point(size = 0.2) +
  theme_graph()

thread_network %>% 
  tidygraph::as_tbl_graph() %>% 
  mutate(children = igraph::degree(thread_network, mode = "out")) %>% 
  #filter(children > 0) %>% 
  ggraph('circlepack', circular = TRUE) + 
  geom_edge_link(alpha = 0.2, check_overlap = TRUE) + 
  geom_node_point(size = 0.2) +
  theme_graph()
```

But we lost too much information. Here we only remove the leafs that are directly connected to the root node. To do so we take advantage of the fact that this kind of directed graph will have $n$ nodes and $n + 1$ edges.

```{r}
hist(full_df$date, breaks = "hour")
```


## Getting multiple threads inside a reddit

Currently, to get all threads we use `RedditExtractor`, which bypasses the API entirely.

```{r}
url_list <- RedditExtractoR::reddit_urls(subreddit = "consulting", 
                                         page_threshold = 100)

glimpse(url_list)
```

We can do it similarly (and faster) using the Python package.

```{python}
subreddit = reddit.subreddit('consulting')
```

```{python}
top = subreddit.top(limit = 1000) ## maximum limit
```

```{python}
date = []
num_comments = []
title = []
link = []

for submission in top:
  title.append(submission.title)
  num_comments.append(submission.num_comments)
  date.append(submission.created_utc)
  link.append(submission.permalink)
```


```{r}
subreddit_df <- tibble(
  title = py$title,
  num_comments = py$num_comments,
  date = as.POSIXct(py$date, origin = "1970-01-01", tz = "UTC"),
  link = py$link
  )

```


## Affiliation network


```{r, eval=FALSE}
unique_users <- consulting %>% 
  filter(num_comments > 0) %>% 
  select(node_data) %>% 
  unnest() %>% 
  distinct(author) %>% 
  filter(author != "[deleted]") %>% 
  pull()

readr::write_rds(unique_users, "consulting_unique_users.rds", 
                 compress = "gz")

```




```{python}
def subreddit_list(name):
  output = []
  for i in reddit.redditor('ConsultingHumor').new():
    output.append(i.subreddit)
  return(output)
```



```{r}
output <- vector("list", length(unique_users))


for (i in 1:length(output)) {
  output[[i]] <- py$subreddit_list(unique_users[[i]]) %>% 
    map_chr(as.character)
}


```










