---
title: "Notes"
author: "andrés castro araújo"
date: "8/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", comments = "")

library(tidyverse)
```


## Get list of threads inside a subreddit

```{r, message=FALSE, warning=FALSE}
# source("dataCollect/downloadURLs.R")
subreddit_urls <- readr::read_csv("dataCollect/subreddit_urls.csv")
```


## Get individual threads

```{r, eval=FALSE}
source("dataCollect/getThread.R")

output <- vector("list", length = nrow(subreddit_urls))
pb <- dplyr::progress_estimated(nrow(subreddit_urls))
for (i in 287:length(output)) {
  
  if (subreddit_urls$num_comments[[i]] == 0) {
    output[[i]] <- NULL
  } else {
    output[[i]] <- extract_thread(path = subreddit_urls$link[[i]])
  }
  try(pb$tick()$print())
  
}

readr::write_rds(output, "dataCollect/thousand_threads.rds", compress = "gz")

subreddit_urls$node_data <- output %>% map(pluck, 1)
subreddit_urls$edge_list <- output %>% map(pluck, 2)

readr::write_rds(subreddit_urls, "dataCollect/consulting.rds", compress = "gz")
```

## Word 2 Vector

```{r}
source("w2v_reddit.R")
subreddit <- readr::read_rds("consulting.rds")
```

```{r}
replacement_string <- c("&" = "and",
                        "/r/" = "",
                        "firms" = "firm")

df <- w2v_preprocess(subreddit, rs = replacement_string)
wordVectors_consulting <- w2v_reddit(df)
readr::write_rds(wordVectors_consulting, "consulting_matrix.rds", compress = "gz")

w2v_cosine_similarity("trust", wordVectors_consulting) %>% View

```



## Affiliation network

```{r}
df <- readr::read_rds("dataCollect/affiliation_network.rds") %>% 
  tibble::enframe(name = "author", value = "subreddit") %>% 
  unnest()
```

Out of the last 100 comments from each of these approx. 8000 users

```{r}
theme_set(hrbrthemes::theme_ipsum())


df %>% 
  count(author, subreddit) %>% 
  arrange(desc(n)) %>% 
  group_by(author) %>% 
  mutate(prop = n / sum(n)) %>% 
  filter(subreddit == "consulting") %>% 
  ggplot(aes(x = prop)) + 
  geom_histogram(color = "black", fill = "steelblue1") +
  scale_x_continuous(labels = scales::percent)
```

The resulting distribution is bimodal. There's a hardcore, but also a casual set of users.

These are the subreddits that were visited the most by the redditors who participate in `consulting`.

```{r}
df %>% 
  count(subreddit) %>% 
  mutate(prop = n/sum(n)) %>% 
  arrange(desc(n)) %>% 
  filter(subreddit != "consulting") %>% 
  top_n(20) %>% 
  ggplot(aes(x = fct_reorder(subreddit, prop), y = prop)) +
  geom_col() + 
  coord_flip() +
  labs(x = NULL, y = NULL) +
  scale_y_continuous(labels = scales::percent)
```

But this doesn't give us a picture of "audiences" or "segments".

To get at that we construct an ___affiliation network___.

```{r}
af_mat <- df %>% 
  count(author, subreddit) %>% 
  filter(n > 2) %>%                             ## arbitrary min n
  tidytext::cast_sparse(author, subreddit, n) 

top_subs <- c("consulting", "politics", "AskReddit", "wallstreetbets", "MBA")

af_mat[1:10, top_subs]
```

This matrix is very, very sparse.

```{r}
sum(af_mat == 0)/ prod(dim(af_mat))
```

99.8% of all cells are empty!

```{r}
library(Matrix)

M <- Matrix::crossprod(af_mat)  ## dual projection
dim(M)

M[top_subs, top_subs]

sum(M == 0)/ prod(dim(M))
```

A histogram of mutual redditors

```{r}
g <- graph_from_adjacency_matrix(M, mode = "undirected", 
                                 weighted = TRUE, diag = FALSE)


## Walktrap clustering algorithm
V(g)$cluster <- igraph::cluster_walktrap(g) %>% 
  igraph::membership() %>% 
  factor() 

top5_clust <- as_data_frame(g, what = "vertices") %>% 
  count(cluster) %>% 
  top_n(5, wt = n) %>% 
  pull(cluster)
  
```

```{r}
tidygraph::as_tbl_graph(g) %>%
  filter(cluster %in% top5_clust) %>% 
  ggraph() +
  geom_node_point(aes(color = as.factor(cluster)), size = 0.5, show.legend = FALSE) +
  geom_edge_diagonal(alpha = 0.01) +
  facet_nodes(~cluster)
```


**Weighting the edges**

The amount of mutual followers between subreddits $i$ and $j$ is logically bounded by the  smallest of the two sets of followers among both of them.

That is, we should weight the strength of each edge (i.e. the amount of mutual redditors) as follows:

$$
w_{i,j}^\prime = \frac{w_{i,j}}{\min(n_i, n_j)}
$$

Doing this will transform each element in the weighted matrix as follows

```{r, eval=FALSE}
## Too slow!

n_vector <- diag(M)
names(n_vector) <- colnames(M)

pb <- dplyr::progress_estimated(prod(dim(M)))

M_weighted <- matrix(NA, nrow = nrow(M), ncol = ncol(M))
for (i in 1:ncol(M)) {
  for (j in 1:nrow(M)) {
    M_weighted[i, j] <- M[i, j] / min(n_vector[c(i, j)])
    try(pb$tick()$print())
  }
}

colnames(M_weighted) <- rownames(M_weighted) <- rownames(M)

```








```{r}
library(igraph)
library(ggraph)


```










